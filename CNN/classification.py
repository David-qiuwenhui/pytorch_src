def main():
    import torch
    import matplotlib.pyplot as plt
    import torch.nn.functional as F

    # å‡æ•°æ®
    n_data = torch.ones(100, 2)  # æ•°æ®çš„åŸºæœ¬å½¢æ€
    # x0 = torch.normal(2 * n_data, 1)  # ç±»å‹0 x data (tensor), shape=(100, 2)
    x0 = torch.normal(mean=2, std=1, size=n_data.shape)
    y0 = torch.zeros(100)  # ç±»å‹0 y data (tensor), shape=(100, )
    x1 = torch.normal(-2 * n_data, 1)  # ç±»å‹1 x data (tensor), shape=(100, 1)
    y1 = torch.ones(100)  # ç±»å‹1 y data (tensor), shape=(100, )

    # æ³¨æ„ x, y æ•°æ®çš„æ•°æ®å½¢å¼æ˜¯ä¸€å®šè¦åƒä¸‹é¢ä¸€æ · (torch.cat æ˜¯åœ¨åˆå¹¶æ•°æ®)
    x = torch.cat((x0, x1), 0).type(torch.FloatTensor)  # FloatTensor = 32-bit floating
    y = torch.cat((y0, y1)).type(torch.LongTensor)  # LongTensor = 64-bit integer

    # ç”»å›¾
    # plt.scatter(x=x.data.numpy()[:, 0], y=x.data.numpy()[:, 1], c=y.data.numpy(), s=100, lw=0, cmap='RdYlGn')
    # plt.show()

    # ç”»å›¾
    # plt.scatter(x.data.numpy(), y.data.numpy())
    # plt.show()

    class Net(torch.nn.Module):  # ç»§æ‰¿ torch çš„ Module
        def __init__(self, n_feature, n_hidden, n_output):
            super(Net, self).__init__()  # ç»§æ‰¿ __init__ åŠŸèƒ½
            self.hidden = torch.nn.Linear(in_features=n_feature, out_features=n_hidden)  # éšè—å±‚çº¿æ€§è¾“å‡º
            self.out = torch.nn.Linear(in_features=n_hidden, out_features=n_output)  # è¾“å‡ºå±‚çº¿æ€§è¾“å‡º

        def forward(self, x):
            # æ­£å‘ä¼ æ’­è¾“å…¥å€¼, ç¥ç»ç½‘ç»œåˆ†æå‡ºè¾“å‡ºå€¼
            x_out = F.relu(self.hidden(x))  # æ¿€åŠ±å‡½æ•°(éšè—å±‚çš„çº¿æ€§å€¼)
            x_out = self.out(x_out)  # è¾“å‡ºå€¼, ä½†æ˜¯è¿™ä¸ªä¸æ˜¯é¢„æµ‹å€¼, é¢„æµ‹å€¼è¿˜éœ€è¦å†å¦å¤–è®¡ç®—
            return x_out

    net = Net(n_feature=2, n_hidden=10, n_output=2)  # å‡ ä¸ªç±»åˆ«å°±å‡ ä¸ª output

    print(net)  # net çš„ç»“æ„
    """
    Net(
      (hidden): Linear(in_features=2, out_features=10, bias=True)
      (out): Linear(in_features=10, out_features=2, bias=True)
    )
    """

    # optimizer æ˜¯è®­ç»ƒçš„å·¥å…·
    optimizer = torch.optim.SGD(params=net.parameters(), lr=0.02)  # ä¼ å…¥ net çš„æ‰€æœ‰å‚æ•°, å­¦ä¹ ç‡
    # ç®—è¯¯å·®çš„æ—¶å€™, æ³¨æ„çœŸå®å€¼!ä¸æ˜¯ one-hot å½¢å¼çš„, è€Œæ˜¯1D Tensor, (batch,)
    # ä½†æ˜¯é¢„æµ‹å€¼æ˜¯2D tensor (batch, n_classes)
    loss_func = torch.nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±å‡½æ•°

    plt.ion()  # ç”»å›¾
    plt.show()
    epochs = 100
    for t in range(epochs):
        out = net(x)  # å–‚ç»™ net è®­ç»ƒæ•°æ® x, è¾“å‡ºé¢„æµ‹å€¼
        print(f"trainâš™ï¸{t + 1}/{epochs}")

        loss = loss_func(out, y)  # è®¡ç®—ä¸¤è€…çš„è¯¯å·®
        print(f"lossğŸ“‰ï¸{t + 1}/{epochs}: loss = {loss}")

        optimizer.zero_grad()  # æ¸…ç©ºä¸Šä¸€æ­¥çš„æ®‹ä½™æ›´æ–°å‚æ•°å€¼
        loss.backward()  # è¯¯å·®åå‘ä¼ æ’­, è®¡ç®—å‚æ•°æ›´æ–°å€¼
        optimizer.step()  # å°†å‚æ•°æ›´æ–°å€¼æ–½åŠ åˆ° net çš„ parameters ä¸Š

        if t % 5 == 0:
            plt.cla()
            # é€šè¿‡ softmax çš„æ¿€åŠ±å‡½æ•°åçš„æœ€å¤§æ¦‚ç‡æ‰æ˜¯é¢„æµ‹å€¼
            prediction = torch.max(F.softmax(out, dim=0), dim=1)[1]
            pred_y = prediction.data.numpy().squeeze()
            target_y = y.data.numpy()
            plt.scatter(x=x.data.numpy()[:, 0], y=x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap='RdYlGn')
            accuracy = sum(pred_y == target_y) / 200.  # é¢„æµ‹ä¸­æœ‰å¤šå°‘å’ŒçœŸå®å€¼ç›¸åŒ
            plt.text(1.5, -4, 'Accuracy=%.2f' % accuracy, fontdict={'size': 20, 'color': 'red'})
            plt.pause(0.1)

        plt.ioff()
        plt.show()

if __name__ == "__main__":
    main()